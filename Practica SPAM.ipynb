{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Correos Electronicos SPAM: Un enfoque con procesamiento de lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cruz David Hernandez Antunez 15111430"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los correos electronicos no deseados en su bandeja de entrada son molestos ya que perturban la rutina del usuario. Es por eso que las cuentas de correo electronico ya tienen un filtro de spam. Dado que es una de las aplicaciones del PLN mas utilizadas vamos a ver como se desarrolla un filtro de spam simple para correos electronicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos las famosas librerias\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insertand los datos\n",
    "full_corpus = pd.read_csv('SMSSpamCollection.tsv', sep='\\t', header= None, names=['label', 'msg_body'])\n",
    "\n",
    "# separando los mensajes en 'ham' y 'spam'\n",
    "ham_text=[]\n",
    "spam_text= []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los N-grammos se usan para modelar el lenguaje en función de la prediccion de palabra, es decir, predice la siguiente palabra de una oracion de palabras N-1 anteriores. Biagrams en la secuencia de 2 palabras de N-gramos que predice la siguiente palabra de una oracion usando la palabra anterior. En lugar de considerar la historia completa de una oracion o una secuencia de palabra en particular, un modelo como bigram puede ser ocupado en terminos de una aproximacion de la historia al ocupar una historia limitada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La identificacion de un mensaje como \"ham\" o \"spam\" es una tarea de clasificacion ya que la variable de destino tiene valores discretos que son \"ham\" o \"spam\". En esta practica se usa el modelo bigram, aunque existen muchas tecnicas avanzadas que se pueden utilizar para este proposito. Para utilizar el modelo bigram para asignar un mensaje dado como \"spam o \"ham\". Hay varios pasos que deben lograrse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.- Inspección y separacion de mensajes en las catergorais \"ham\" y \"spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, el conjunto de datos se debe inspeccionar para ocuparlo y abordarlo para lograr la tarea. El formato de los datos dados, la cantidad de datos proporcionados, la naturaleza de los datos se incluyen en esta inspeccion para identificar la mejor aproximacion posible para la tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El corpus de mensajes dado ha marcado cada mensaje como ham o spam. Ademas hay 5568 mensajes en un DataFrame escrito en ingles que no son objets nulos. Por lo tanto, el archivo tsv se puede leer usando DataFrame en python para clasificar esos mensajes de acuerdo con el indicador dado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_msgs():\n",
    "    for index, column in full_corpus.iterrows():\n",
    "        label = column[0]\n",
    "        message_text= column[1]\n",
    "        if label=='ham':\n",
    "            ham:text.append(message_text)\n",
    "        elif label=='spam':\n",
    "            spam_text.append(message_text)\n",
    "separate_msgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.- Preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El preprocesamiento es la tarea de realizar los pasos de preparacion en el corpus de texto sin formato para completar de manera eficiente una extracción de texto o procesamiento de lenguaje natural o cualquier otra tarea que incluya texto sin formato. El preprocesamiento de texto consta de varios pasos, aunque algunos d ellos pueden no aplicarse a una tarea en particular debido a la naturaleza del conjunto de datos disponible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta tarea, el preprocesamiento de texto incluye los siguientes pasos de acuerdo al conjunto de datos: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminacion de signos de puntuacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminacion de los signos de puntuacion de los mensajes de correro electronico\n",
    "def remove_msg_punctuations(email_msg):\n",
    "    puntuation_removed_msg = \"\".join([word for word in email_msg if word not in string.punctuation])\n",
    "    return puntuation_removed_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir a minusculas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir a minusculas: La conversacion de todos los caracteres de texto en un contexto común, como los soportes en minúsculas, impide identificar dos palabras de manera diferente donde una está en minuscula y la otra no. Por ejemplo, \"Primero\" y \"primero\" deben identificarse iguales. Por lo tanto poner en minuscula todos los caracteres facilita la tarea. Ademas, Las palabras de detencion tambien esta en minusculas, Por lo que esto también haría posble eliminar palabras de detencion mas adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing: La tokenizacion es la tarea de dividir el texto en partes significativas, es decir, tokens que incluyen oraciones y palabras. un token se puede considerar como una instancia de una secuenta de caracteres en un texto particular que se agrupa para proporcionar una unidad semantica util para su posterior procesamiento. En esta tarea, la tokenizacion de palabras se realiza combinando espacios en blanco entre palabras como delimitador. Esto se logra en Python usando expresiones regulares para dividir una cadena en subcadenas con la funcion split(), que es un tokenizador basico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierte el texto en minusculo y tokenizing de palabras\n",
    "def tokenize_into_words(text):\n",
    "    tokens=re.split('\\W+', text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras lematizantes: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La derivacion es el proceso de eliminar afijos(sufijos, prefijos, infijos,circunfijos) de una palabra para obtener su raiz de palabra. Aunque la lematizacion esta relacionada con la derivacion, difiere ya que la lematizacion puede capturar formas canonicas basadas en el lema de una palabra. La lematizacion ocupa un vocabulario y un analisis morgologico de las palabras que lo hacen mas rapido y preciso que la derivacion. WordNetLemmatizer ha logrado la lematizacion en lenguaje python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lematizing\n",
    "word_lemmatizer= WordNetLemmatizer ()\n",
    "def lemmatization(tokenized_words):\n",
    "    lemmatized_text= [word_lemmatizer.lemmatize(word) for word in tokenized_words]\n",
    "    return ''.jouin(lemmatized_text)\n",
    "def preprocessing_msgs(corpus):\n",
    "    categorized_text = pd.DataFrame(corpus)\n",
    "    categorized_text['non_punc_message_body'] = categorized_text[0].apply(lambda msg: remove_msg_punctuations(msg))\n",
    "    categorized_text['tokenized_msg_body'] = categorized_text['non_punc_message_body'].apply(lambda msg: tokenize_into_words(msg.lower()))\n",
    "    categorized_text['lemmatized_msg_words'] = categorized_text['tokenized_msg_body'].apply(lambda word_list: lemmatization(word_list))\n",
    "    return categorized_text['lemmatized_msg_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.- Extracción de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de la etapa de preprocesamiento, las caracteristicas deben extraerse del texto. Las caracteristicas son las unidades que admiten la tarea de clasificacion, y las bigrams son las caracteristicas en esta tarea de clasificacion de mensajes. Los bigrams o las caracteristicas se extraen del texto preprocesado. inicialmente, los unigramas se adquieren y luego esos unigramas se utilizan para obtener los unigramas en cada corpus (\"ham\" y \"spa,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraccion de caracteristicas. Ejemplo: n-grams\n",
    "def feature_extraction(preprocessed_text):\n",
    "    bigrams=[]\n",
    "    unigrams_lists=[]\n",
    "    for msg in preprocessed_text:\n",
    "        #agregando end of and start of al mensaje\n",
    "        msg='<s>'+ msg+'</s>'\n",
    "        unigrams_lists.append(msg.split())\n",
    "    unigrams = [uni_list for sub_list in unigrams_lists for uni_list in sub_list]\n",
    "    bigrams.extend(nltk.bigrams(unigrams))\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Eliminacion de Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay ciertas palabras en un idioma (se utiliza inglés en la practica) que son necesarias para una oracion o una secuencia de palabras, aunque no cintribuyen al significado de una frase considerada. la biblioteca del kit de herramientas de lenguaje natural (NLTK) en python proporciona palabras de detencion comunes para algunos idiomas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de eliminar las palabras de detencion en el paso de preprocesamiento, se realiza despues de extraer las caracteristicas del corpus para evitar la ausencia de bigrams con palabras de una parada ((\"use\",\"your\"),(\"to\",\"win\")) al adiquierir las funciones, ya que tienen un impacto en el resultado final de la aplicacion. Las palabras de detencion se pueden ignorar en esta recuperacion de informacion orientada a palabras clave debido a su efecto en la precision de la recuperacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminando bigrams solo con stop words\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "def filter_stopwords_bigrams(bigram_list):\n",
    "    filtered_bigrams=[]\n",
    "    for bigram in bigram_list:\n",
    "        if bigram[0] in stopwords and bigram[1] in stopwords:\n",
    "            continue\n",
    "        filtered_bigrams.appends(bigram)\n",
    "        return filtered_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.- Obtener distribucion de frecuencia de caracteristicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribucion de frecuencia se utiliza para obtener la frecuencia de aparicion de cada elemento de vocabulario en un texto determinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adquiriendo la frecuencia de caracteristicas\n",
    "def ham_bigram_feature_frequency():\n",
    "    # frecuencia de caracteristicas para ensajes ham\n",
    "    ham_bigrams=feature_extraction(preprocessing_msgs(ham_text))\n",
    "    ham_bigram_frequency = nltk.FreqDist(filter_stopwords_bigrams(ham_bigrams))\n",
    "    return ham_bigram_frequency\n",
    "def spam_bigram_feature_frequency():\n",
    "    # frecuencia de caracteristicas para mensaje spam\n",
    "    spam_bigrams= feature_extraction(preprocessing_msgs(spam_text))\n",
    "    spam_bigram_frequency = nltk.FreqDist(filter_stopwords_bigrams(spam_bigrams))\n",
    "    return spam_bigram_frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.- Construyendo un modelo para la predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo para clasificar un mensaje dado como \"ham\" o \"spam\" se ha abordado calculando las probabilidads de bigram dentro de cada corpus. Inicialmente, el mensaje dado debe procesarse previamente para avanzar con clasificacion, incluida la eliminacion de signos de puntuacion, el cambio de todos los caracteres a minusculas, la tokenizacion y la lematizacion. Luego los bigrams se extraen del texto preprocesado para calcular finalmente la probabilidad de que el texto esté en cada corpus \"ham\" o \"spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-c9fd4ff4cad7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\\"'\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'\\\" es un mensaje Spam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m \u001b[0mbigram_probability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Click here,  ..to win an iphone 11 pro max'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[0mbigram_probability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Win a brand new car '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-c9fd4ff4cad7>\u001b[0m in \u001b[0;36mbigram_probability\u001b[1;34m(message)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mbigrams_for_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatized_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Eliminamos stop words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mham_unigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessing_msgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mham_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mspam_unigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessing_msgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspam_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Frecuencias de bigrams extraidas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-7a1b9e001fd6>\u001b[0m in \u001b[0;36mpreprocessing_msgs\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocessing_msgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mcategorized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mcategorized_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'non_punc_message_body'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategorized_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mremove_msg_punctuations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mcategorized_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized_msg_body'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategorized_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'non_punc_message_body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtokenize_into_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mcategorized_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lemmatized_msg_words'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategorized_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized_msg_body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mword_list\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2978\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2979\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2980\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2982\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2899\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Calculando probabilidades del bigram\n",
    "def bigram_probability(message):\n",
    "    probability_h = 1\n",
    "    probability_s = 1\n",
    "    # Preprocesando los mensaje de entrada\n",
    "    punc_removed_message = \"\".join(word for word in message if word not in string.punctuation)\n",
    "    punc_removed_message = '<s> ' +punc_removed_message +' </s>'\n",
    "    tokenized_msg = re.split('\\s+', punc_removed_message)\n",
    "    lemmatized_msg = [word_lemmatizer.lemmatize(word)for word in tokenized_msg]\n",
    "    # bigrams para el mensaje\n",
    "    bigrams_for_msg = list(nltk.bigrams(lemmatized_msg))\n",
    "    # Eliminamos stop words\n",
    "    ham_unigrams = [word for word in feature_extraction(preprocessing_msgs(ham_text)) if word not in stopwords]\n",
    "    spam_unigrams = [word for word in feature_extraction(preprocessing_msgs(spam_text)) if word not in stopwords]\n",
    "    # Frecuencias de bigrams extraidas\n",
    "    ham_frequency = ham_bigram_feature_frequency()\n",
    "    spam_frequency  = spam_bigram_feature_frequency()\n",
    "    print('========================== Calculando Probabilidades ==========================')\n",
    "    print('----------- Frecuencias Ham ------------')\n",
    "    for bigram in bigrams_for_msg:\n",
    "        # probabilidad de la primera palabra en bigram\n",
    "        ham_probability_denominator = 0\n",
    "        # probabilidad de bigram (suavizado) \n",
    "        ham_probability_of_bigram = ham_frequency[bigram] + 1\n",
    "        print(bigram, ' ocurre ', ham_probability_of_bigram)\n",
    "        for (first_unigram, second_unigram) in filter_stopwords_bigrams(ham_unigrams):\n",
    "            ham_probability_denominator += 1\n",
    "            if(first_unigram == bigram[0]):\n",
    "                ham_probability_denominator += ham_frequency[first_unigram, second_unigram]\n",
    "        probability = ham_probability_of_bigram / ham_probability_denominator\n",
    "        probability_h *= probability\n",
    "    print('\\n')\n",
    "    print('----------- Frecuencias Spam ------------')\n",
    "    for bigram in bigrams_for_msg:\n",
    "        # probabilidad de la primera palabra en bigram\n",
    "        spam_probability_denominator = 0\n",
    "        # probabilidad de bigram (suavizado) \n",
    "        spam_probability_of_bigram = spam_frequency[bigram] + 1\n",
    "        print(bigram, ' ocurre ', spam_probability_of_bigram)\n",
    "        for (first_unigram, second_unigram) in filter_stopwords_bigrams(spam_unigrams):\n",
    "            spam_probability_denominator += 1\n",
    "            if(first_unigram == bigram[0]):\n",
    "                spam_probability_denominator += spam_frequency[first_unigram, second_unigram]\n",
    "        probability = spam_probability_of_bigram / spam_probability_denominator\n",
    "        probability_s *= probability\n",
    "    print('\\n')\n",
    "    print('Probabilidad Ham: ' +str(probability_h))\n",
    "    print('Probabildiad Spam: ' +str(probability_s))\n",
    "    print('\\n')\n",
    "    if(probability_h >= probability_s):\n",
    "        print('\\\"' +message +'\\\" es un mensaje Ham')\n",
    "    else:\n",
    "        print('\\\"' +message +'\\\" es un mensaje Spam')\n",
    "    print('\\n')\n",
    "bigram_probability('Click here,  ..to win an iphone 11 pro max')\n",
    "bigram_probability('Win a brand new car ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
